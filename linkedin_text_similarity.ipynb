{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linkedin_text_similarity.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKbxCsviGhFXHj94J73LDw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plchuaa/gordonchu/blob/master/linkedin_text_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYzL6k2PR2WF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X37dQsGcR8Lh",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing by get text from json by some simple cleaning and store at a dataframe for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvnHm6chRtl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_cleaning(text):\n",
        "    if len(text) > 0:\n",
        "        text = text[0]\n",
        "        text = re.sub(r\"\\\\n\", \" \", text)\n",
        "        # text = re.sub(r\"\\.\\.\\.\", \" \", text)\n",
        "        text = re.sub(r\"\\.+\\.+\", \" \", text)\n",
        "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "        text = remove_emoji(text)\n",
        "        text = remove_url(text)\n",
        "        text = remove_newline(text)\n",
        "    else:\n",
        "        text = \"\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_name(json_dictionairy):\n",
        "    data = json_dictionairy[\"current\"][0]\n",
        "    name = re.findall(r'(?:\\n +)([\\S\\s]+)(?:\\n +Status is)', data)\n",
        "    name = text_cleaning(name)\n",
        "    return name\n",
        "\n",
        "\n",
        "def get_info(json_dictionairy):\n",
        "    data = json_dictionairy[\"current\"][0]\n",
        "    info = re.findall(r'(?:See contact info(?:\\n +)*[a-z0-9 A-Z+]+(?:\\n +)*)([\\s\\S]+)', data)\n",
        "    info = text_cleaning(info)\n",
        "    return info\n",
        "\n",
        "\n",
        "def get_summary(json_dictionairy):\n",
        "    data = json_dictionairy[\"summary\"]\n",
        "    summary = text_cleaning(data)\n",
        "    return summary\n",
        "\n",
        "\n",
        "def list_cleaning(data):\n",
        "    lists = []\n",
        "    for exp in data:\n",
        "        if isinstance(exp, list):\n",
        "            for e in exp:\n",
        "                lists.append(text_cleaning([e]))\n",
        "        else:\n",
        "            lists.append(text_cleaning([exp]))\n",
        "\n",
        "    return (\" \").join(lists) if len(lists) > 0 else \"\"\n",
        "\n",
        "\n",
        "def get_experience(json_dictionairy):\n",
        "    data = json_dictionairy[\"experience\"]\n",
        "    experience = list_cleaning(data)\n",
        "    return experience\n",
        "\n",
        "\n",
        "def get_education(json_dictionairy):\n",
        "    data = json_dictionairy[\"education\"]\n",
        "    education = list_cleaning(data)\n",
        "    return education\n",
        "\n",
        "\n",
        "def get_license(json_dictionairy):\n",
        "    data = json_dictionairy[\"licenses\"]\n",
        "    licenses = list_cleaning(data)\n",
        "    return licenses\n",
        "\n",
        "\n",
        "def get_skill(json_dictionairy):\n",
        "    data = json_dictionairy[\"skills\"]\n",
        "    skills = list_cleaning(data)\n",
        "    return skills\n",
        "\n",
        "\n",
        "def get_tech(json_dictionairy):\n",
        "    data = json_dictionairy[\"tech\"]\n",
        "    tech = list_cleaning(data)\n",
        "    return tech\n",
        "\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_url(text):\n",
        "    text = re.sub(r'[a-zA-z]+://[^\\s]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[a-zA-Z]+(\\.[a-zA-Z]+)+', '', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_newline(text):\n",
        "    text = re.sub(r\"\\n\", \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Text Preprocessing(remove stopwords,punctuations).\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# build a list of stopwords to use to filter\n",
        "stopwords = list(STOP_WORDS)\n",
        "stopwords.append(\"see more\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3t3gmEkSVwp",
        "colab_type": "code",
        "outputId": "4d126bad-35b1-495c-b02f-53bf46ce7982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Get data from drive\n",
        "drive.mount('/content/drive',force_remount=False)\n",
        "os.chdir(\"/content/drive/My Drive/\")\n",
        "\n",
        "\n",
        "# Or change os.scandir('path-to-the-linkedin-dataset')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rKsw_qi9zZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "profile_df  = pd.DataFrame(columns=[\"profile_name\",\"profile_detail\", \"profile_info\",\"profile_summary\",\"profile_experience\",\n",
        "                                         \"profile_education\",\"profile_license\",\"profile_skill\",\"profile_tech\"])\n",
        "\n",
        "with os.scandir('./linkedin/') as entries:\n",
        "    for each in entries:\n",
        "        file = open(each, 'r', encoding=\"utf-8\")\n",
        "        profile_data = json.load(file)\n",
        "\n",
        "        profile_name = get_name(profile_data)\n",
        "        profile_info = get_info(profile_data)\n",
        "        profile_summary = get_summary(profile_data)\n",
        "        profile_experience = get_experience(profile_data)\n",
        "        profile_education = get_education(profile_data)\n",
        "        profile_license = get_license(profile_data)\n",
        "        profile_skill = get_skill(profile_data)\n",
        "        profile_tech = get_tech(profile_data)\n",
        "        profile_detail = profile = profile_info + profile_summary + profile_experience + profile_education + profile_license + profile_skill + profile_tech\n",
        "\n",
        "        profile_to_row = {}\n",
        "        profile_to_row[\"profile_name\"] = profile_name\n",
        "        profile_to_row[\"profile_info\"] = profile_info\n",
        "        profile_to_row[\"profile_summary\"] = profile_summary\n",
        "        profile_to_row[\"profile_experience\"] = profile_experience\n",
        "        profile_to_row[\"profile_education\"] = profile_education\n",
        "        profile_to_row[\"profile_skill\"] = profile_skill\n",
        "        profile_to_row[\"profile_license\"] = profile_license\n",
        "        profile_to_row[\"profile_tech\"] = profile_tech\n",
        "        profile_to_row[\"profile_detail\"] = profile_detail\n",
        "\n",
        "        profile_df = profile_df.append(profile_to_row, ignore_index=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi6wLnN3fRQD",
        "colab_type": "code",
        "outputId": "dc4902d0-0043-4f31-991b-bb467977bf74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        }
      },
      "source": [
        "profile_df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>profile_name</th>\n",
              "      <th>profile_detail</th>\n",
              "      <th>profile_info</th>\n",
              "      <th>profile_summary</th>\n",
              "      <th>profile_experience</th>\n",
              "      <th>profile_education</th>\n",
              "      <th>profile_license</th>\n",
              "      <th>profile_skill</th>\n",
              "      <th>profile_tech</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abby Tang</td>\n",
              "      <td>Business Strategist skilled in Joint Marketing...</td>\n",
              "      <td>Business Strategist skilled in Joint Marketing...</td>\n",
              "      <td>Business Strategist skilled in Joint Marketin...</td>\n",
              "      <td>APCJ Director, Cloud Field Operations Company...</td>\n",
              "      <td>University of California, Irvine Degree Name ...</td>\n",
              "      <td></td>\n",
              "      <td>Business Alliances   Go-to-market Strategy   ...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ai Shinozaki</td>\n",
              "      <td>My nick name is Ai Shinozaki. I am a PG Girl S...</td>\n",
              "      <td>My nick name is Ai Shinozaki. I am a PG Girl S...</td>\n",
              "      <td>My nick name is Ai Shinozaki. I am a PG Girl ...</td>\n",
              "      <td>PG Model Company Name ABC PG Model Dates Empl...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Target Orientation   Reading Comprehension   ...</td>\n",
              "      <td>Other Skills Sexy PG Show Sexy Girl Face &amp; Bo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dr Alan Chow MIET MIEEE MAUSIMM</td>\n",
              "      <td>Founder of a disruptive project with strong Gl...</td>\n",
              "      <td>Founder of a disruptive project with strong Gl...</td>\n",
              "      <td>Founder of a disruptive project with strong G...</td>\n",
              "      <td>Chairman, Investment Committee Company Name C...</td>\n",
              "      <td>Renmin University of China Degree Name Doctor...</td>\n",
              "      <td></td>\n",
              "      <td>Investments   Private Equity   Corporate Fina...</td>\n",
              "      <td>Industry Knowledge Equities See 25 endorsemen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Abel Cheung</td>\n",
              "      <td>Administrator in a variety of coorporate netwo...</td>\n",
              "      <td>Administrator in a variety of coorporate netwo...</td>\n",
              "      <td>Administrator in a variety of coorporate netw...</td>\n",
              "      <td>Network Architect Company Name Next Media Ani...</td>\n",
              "      <td>AccessData BootCamp &amp; FTK training Field Of S...</td>\n",
              "      <td>Novell Certified Linux Administrator (Novell ...</td>\n",
              "      <td>Linux   Apache   Ubuntu</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ai autodata</td>\n",
              "      <td>Managing Director Company Name</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Managing Director Company Name</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Xin LI</td>\n",
              "      <td>I am a second-year . student at CUHK. Prior to...</td>\n",
              "      <td>I am a second-year . student at CUHK. Prior to...</td>\n",
              "      <td>I am a second-year . student at CUHK. Prior t...</td>\n",
              "      <td>Research Intern, NLP Group@Tencent AI Lab Com...</td>\n",
              "      <td>Field Of Study Faculty of Engineering Dates ...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Wong Loretta</td>\n",
              "      <td>Company Name Google Total Duration 4 yrs 3 m...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Company Name Google Total Duration 4 yrs 3 mo...</td>\n",
              "      <td>The University of Hong Kong Degree Name Bache...</td>\n",
              "      <td></td>\n",
              "      <td>Marketing Strategy   Search Engine Technology...</td>\n",
              "      <td>Industry Knowledge Business Strategy See 3 en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Worldmoney Blockchain News</td>\n",
              "      <td>Company Name Worldmoney Blockchain Financial...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Company Name Worldmoney Blockchain Financial ...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Wayne Liu</td>\n",
              "      <td>Technology professional with 10 years experien...</td>\n",
              "      <td>Technology professional with 10 years experien...</td>\n",
              "      <td>Technology professional with 10 years experie...</td>\n",
              "      <td>Technology Specialist Company Name 3Revolutio...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Digital Printing   Color Management   Printer...</td>\n",
              "      <td>Industry Knowledge R&amp;D See 2 endorsements for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Ray (Wen Jie) LIEW</td>\n",
              "      <td>Ray Liew is the Data and AI solution specialis...</td>\n",
              "      <td>Ray Liew is the Data and AI solution specialis...</td>\n",
              "      <td>Ray Liew is the Data and AI solution speciali...</td>\n",
              "      <td>Data and AI Solution Specialist (SSP) Company...</td>\n",
              "      <td>HKUST Business School Degree Name Bachelor of...</td>\n",
              "      <td>Insight Selling Issued date and, if applicabl...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                        profile_name  ...                                       profile_tech\n",
              "0                          Abby Tang  ...                                                   \n",
              "1                       Ai Shinozaki  ...   Other Skills Sexy PG Show Sexy Girl Face & Bo...\n",
              "2    Dr Alan Chow MIET MIEEE MAUSIMM  ...   Industry Knowledge Equities See 25 endorsemen...\n",
              "3                        Abel Cheung  ...                                                   \n",
              "4                        ai autodata  ...                                                   \n",
              "..                               ...  ...                                                ...\n",
              "95                            Xin LI  ...                                                   \n",
              "96                      Wong Loretta  ...   Industry Knowledge Business Strategy See 3 en...\n",
              "97        Worldmoney Blockchain News  ...                                                   \n",
              "98                         Wayne Liu  ...   Industry Knowledge R&D See 2 endorsements for...\n",
              "99                Ray (Wen Jie) LIEW  ...                                                   \n",
              "\n",
              "[100 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4wWOm87jDci",
        "colab_type": "text"
      },
      "source": [
        "Possible direction:\n",
        "1. further split the profile into parts(info + summary, exp, tech+skill+license)\n",
        "\n",
        "2. modify ways to calculate similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLrVF0DDfRg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spacy tokenizer to remove stopwords and punctuation, lemma words\n",
        "def spacy_tokenizer(text):\n",
        "    parser = English()\n",
        "    mytokens = parser(text)\n",
        "    mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n",
        "    mytokens = [word for word in mytokens if word not in stopwords]\n",
        "    mytokens = [word for word in mytokens if word.isalpha() or word.isnumeric() or len(word)>1 ]\n",
        "    # mytokens = [word for word in mytokens if word not in string.punctuation]\n",
        "    return mytokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvE24EJTEIMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example1 = profile_df[\"profile_detail\"][0]\n",
        "example1 = spacy_tokenizer(example1)\n",
        "# print(example1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwFc0kOshzPg",
        "colab_type": "text"
      },
      "source": [
        "Different way to get similarity matrix \n",
        "\n",
        "**1. spacy_tokenizer + genism(tfidf)**\n",
        "\n",
        "2. spacy_tokenizer + sklearn tfidfvectorizer + cosine similarity\n",
        "\n",
        "3. spacy_tokenizer + spacy similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRmPoSCcyBej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# target_profile = input(\"something here: \")\n",
        "target_profile = \"My nick name is Ai Shinozaki. I am a PG Girl Show. I can make Sexy Walk show, Sexy Face Funny Show or whatever, I can speak Japanese, Mandarin and English. I love food, sports, music, movies. I like to travel and picnic to different countries.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHQ6b6vrPEo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pprint\n",
        "from gensim import corpora\n",
        "\n",
        "# Lowercase each document, split it by white space and filter out stopwords\n",
        "texts = [spacy_tokenizer(text) for text in profile_df[\"profile_detail\"]]\n",
        "\n",
        "# Count word frequencies\n",
        "from collections import defaultdict\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "# Only keep words that appear more than once\n",
        "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psTNMfpLzHyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = corpora.Dictionary(processed_corpus)\n",
        "# pprint.pprint(dictionary.token2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaxV4zB-zLL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
        "# pprint.pprint(bow_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9SPHSGsnX5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "# print(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoeuhjlSnfXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim import models\n",
        "\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "# for doc in corpus_tfidf:\n",
        "#     print(doc) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp1Sw44yogG6",
        "colab_type": "code",
        "outputId": "9c0aed93-6c86-4f73-b042-d741eb62a85b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)  # initialize an LSI transformation\n",
        "corpus_lsi = lsi_model[corpus_tfidf] \n",
        "lsi_model.print_topics(2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.219*\"endorsements\" + 0.176*\"date\" + 0.149*\"marketing\" + 0.137*\"microsoft\" + 0.130*\"title\" + 0.124*\"issued\" + 0.121*\"data\" + 0.117*\"expiration\" + 0.116*\"endorsement\" + 0.113*\"sales\"'),\n",
              " (1,\n",
              "  '-0.308*\"date\" + 0.288*\"marketing\" + -0.221*\"credential\" + -0.219*\"issued\" + -0.209*\"expiration\" + 0.177*\"endorsements\" + -0.168*\"architect\" + -0.167*\"aws\" + -0.160*\"certified\" + -0.125*\"authority\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RizCvch8pE5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for doc, as_text in zip(corpus_lsi, profile_df[\"profile_detail\"]):\n",
        "#     print(doc, as_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0incPm7pIAE",
        "colab_type": "code",
        "outputId": "8cfe2b2c-c347-48e8-ecc4-815b4d2ac8b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from gensim import similarities\n",
        "index = similarities.MatrixSimilarity(lsi_model[corpus])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo_F18Xvs_Iz",
        "colab_type": "code",
        "outputId": "fe52b34b-a6bd-43db-eaf9-08894ce11298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "index.save('/tmp/deerwester.index')\n",
        "index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyk0y7ostibP",
        "colab_type": "code",
        "outputId": "3906601e-b58a-4f9d-de9c-dc1b7cd6b5ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vec_bow = dictionary.doc2bow(spacy_tokenizer(target_profile))\n",
        "vec_lsi = lsi_model[vec_bow]  # convert the query to LSI space\n",
        "print(vec_lsi) # [(0, 4.325712071055434), (1, 0.10321814096913436)]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.22168110842854594), (1, 0.09817372435877599)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tszQLx93tRR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
        "sims_list = list(enumerate(sims))\n",
        "# print(sims_list)  # print (document_number, document_similarity) 2-tuples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqb_UZqbtUrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
        "# sims"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2n4TnF8usMA",
        "colab_type": "code",
        "outputId": "fa39abd8-e400-49e3-bedd-b1e2de5d9169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(\"Top 5 similar profile: \")\n",
        "for each in sims[0:5]:\n",
        "    print(each,profile_df[\"profile_name\"][each[0]])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 5 similar profile: \n",
            "(31, 0.9999729)  Dr. Lawrence Ma\n",
            "(18, 0.99995357)  Bess Chung\n",
            "(83, 0.9999157)  Shirley Kwok\n",
            "(50, 0.9997908)  Jean Wang\n",
            "(44, 0.99956244)  Ian Lee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAC5_XYG2OpN",
        "colab_type": "text"
      },
      "source": [
        "Different way to get similarity matrix \n",
        "\n",
        "1. spacy_tokenizer + genism(tfidf)\n",
        "\n",
        "**2. spacy_tokenizer + sklearn tfidfvectorizer + cosine similarity**\n",
        "\n",
        "3. spacy_tokenizer + spacy similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYDELQni9nyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documents = [(\" \").join(spacy_tokenizer(text)) for text in profile_df[\"profile_detail\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQCq5FtU9oQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Create the Document Term Matrix\n",
        "count_vectorizer = TfidfVectorizer()\n",
        "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
        "doc_term_matrix = sparse_matrix.todense()\n",
        "df = pd.DataFrame(doc_term_matrix, \n",
        "                  columns=count_vectorizer.get_feature_names(), \n",
        "                  index=profile_df[\"profile_name\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQDue4vj9tkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute Cosine Similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sims_matrix = cosine_similarity(df, df)\n",
        "# print(sims_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV4UwQjTJqQu",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_BetkpPXok9Z"
      },
      "source": [
        "Different way to get similarity matrix \n",
        "\n",
        "1. spacy_tokenizer + genism(tfidf)\n",
        "\n",
        "2. spacy_tokenizer + sklearn tfidfvectorizer + cosine similarity\n",
        "\n",
        "**3. spacy_tokenizer + spacy similarity**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjZmchX_fTEe",
        "colab_type": "code",
        "outputId": "04a39028-bd78-4952-ceb7-09ef79300a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2OU3dumHI9a",
        "colab_type": "code",
        "outputId": "4c994827-cb82-40f0-de3e-0fbab947c608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "token1 = nlp(\"i love dog.\")\n",
        "token5 = nlp(\"i love dog\")\n",
        "token6 = nlp(\"dog love me.\")\n",
        "token2 = nlp(\"i love dog?\")\n",
        "token3 = nlp(\"i hate dog.\")\n",
        "token4 = nlp(\"cat love dog.\")\n",
        "token7 = nlp(\"rabbit like carrot.\")\n",
        "token1.similarity\n",
        "print(token1.vector_norm,token2.vector_norm,token3.vector_norm,token4.vector_norm,token5.vector_norm,token6.vector_norm,token7.vector_norm)\n",
        "# 4.431595721931874 4.507516618461838 4.391567403021321 4.6381735996661915 5.044537372524752 4.369449490570207 3.8596050346561275"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.431595721931874 4.507516618461838 4.391567403021321 4.6381735996661915 5.044537372524752 4.369449490570207 3.8596050346561275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL73YmmK_k85",
        "colab_type": "code",
        "outputId": "935ad223-95f7-4fa4-80dc-ba981a3cbdce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# get profile detail and tokenize them with spacy\n",
        "documents = [(\" \").join(spacy_tokenizer(text)) for text in profile_df[\"profile_detail\"]] \n",
        "\n",
        "target_profile = (\" \").join(spacy_tokenizer(target_profile))\n",
        "target_token = nlp(target_profile)\n",
        "\n",
        "sims_list = []\n",
        "i = 0\n",
        "for profile in documents:\n",
        "    profile_token = nlp(profile)\n",
        "    sims_list.append((i,profile_token.similarity(target_token)))\n",
        "    i += 1\n",
        "# print(sims_list)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D67k2vGrsFnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sims_list = sorted(sims_list, key=lambda item: -item[1])\n",
        "# print(sims_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awY_WzeEs0CC",
        "colab_type": "code",
        "outputId": "2fc0324a-1191-41a1-caa6-de023855e7e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(\"Top 5 similar profile: \")\n",
        "for each in sims_list[0:5]:\n",
        "    print(each,profile_df[\"profile_name\"][each[0]])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 5 similar profile: \n",
            "(1, 0.96920621010979)  Ai Shinozaki\n",
            "(90, 0.706554777487361)  Vincent Shiu\n",
            "(99, 0.6873498148382473)  Ray (Wen Jie) LIEW\n",
            "(82, 0.672471352296247)  Sherman Lee\n",
            "(96, 0.671801147121337)  Wong Loretta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJuERy2ks4ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}